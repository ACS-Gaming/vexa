# Baseline Configuration for WhisperLive Optimization Testing
# This represents the default/reference configuration

server:
  ws_url: "ws://localhost:9090/ws"  # For local testing
  # ws_url: "ws://whisperlive-server:9090/ws"  # For Docker container
  # auth_header: "X-API-Key: your-api-key-here"  # Optional
  language: "en"
  model: "medium"
  # Server-side parameters (set via environment variables or server startup flags)
  # compute_type: "float16"        # Options: float16, int8, int8_float16
  # beam_size: 1                   # Beam search size
  # num_workers: 4                 # Number of worker threads
  # min_audio_s: 1.0              # Minimum audio length for transcription
  # vad_onset: 0.5                # VAD onset threshold
  # vad_no_speech_thresh: 0.5     # VAD no-speech threshold

run:
  concurrency: 2                  # Number of concurrent connections (optimal from previous tests)
  frame_ms: 20                    # Audio frame size in milliseconds
  warmup_s: 2                     # Warmup duration in seconds (reduced for faster testing)
  run_s: 60                       # Main test duration in seconds (reduced for faster testing)
  cooldown_s: 2                   # Cooldown duration in seconds (reduced for faster testing)
  repeat_audio: true              # Repeat audio samples if needed
  shuffle_audio: true             # Shuffle audio sample order
  per_conn_seed: true             # Use fixed seed for reproducibility

data:
  manifest: "data/manifest.csv"   # Path to audio manifest file

metrics:
  lambda: 0.5                     # Penalty weight for standard deviation
  latency_slo: 2.0               # Latency SLO threshold in seconds
  drop_slo: 0.02                 # Drop rate SLO threshold (2%)
  gpu_sample_s: 1.0              # GPU metrics sampling interval

quality:
  enable_simple: true             # Enable simple text quality metrics
  enable_llm: false              # Enable LLM judge assessment
  llm_provider: "openai"         # LLM provider (openai, anthropic)
  llm_model: "gpt-4o-mini"       # LLM model to use
  judge_prompt_path: "configs/judge_prompt.md"  # Path to judge prompt template

# Advanced settings
advanced:
  max_concurrent_connections: 1000  # Max concurrent client connections
  connection_jitter_ms: 100       # Jitter between connection starts (ms)
  metrics_sample_rate: 1.0        # Metrics sampling rate (Hz)
  steady_state_window: 20         # Steady state detection window (seconds)
  steady_state_threshold: 0.03    # Steady state threshold (3% change)
