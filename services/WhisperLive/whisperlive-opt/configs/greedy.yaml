# Greedy Decoding Configuration for WhisperLive Optimization Testing
# Optimized for speed with beam_size=1, best_of=1, temperature=0.0

server:
  ws_url: "ws://localhost:9090/ws"
  # auth_header: "X-API-Key: your-api-key-here"  # Optional
  language: "en"
  model: "small"
  # Greedy decoding parameters (set via environment variables)
  # compute_type: "int8_float16"   # Faster computation with minimal quality loss
  # beam_size: 1                   # Greedy decoding (no beam search)
  # best_of: 1                     # Single best result
  # temperature: 0.0               # Deterministic output
  # num_workers: 6                 # More workers for faster processing
  # min_audio_s: 0.5              # Lower minimum for faster response
  # vad_onset: 0.3                # More sensitive VAD for faster detection
  # vad_no_speech_thresh: 0.4     # Adjusted VAD thresholds

run:
  concurrency: 32                 # Number of concurrent connections
  frame_ms: 20                    # Audio frame size in milliseconds
  warmup_s: 15                    # Shorter warmup for greedy decoding
  run_s: 120                      # Main test duration in seconds
  cooldown_s: 5                   # Shorter cooldown
  repeat_audio: true              # Repeat audio samples if needed
  shuffle_audio: true             # Shuffle audio sample order
  per_conn_seed: true             # Use fixed seed for reproducibility

data:
  manifest: "data/manifest.csv"   # Path to audio manifest file

metrics:
  lambda: 0.3                     # Lower penalty weight (expect more variance)
  latency_slo: 1.5               # Stricter latency SLO for greedy decoding
  drop_slo: 0.01                 # Stricter drop rate SLO (1%)
  gpu_sample_s: 1.0              # GPU metrics sampling interval

quality:
  enable_simple: true             # Enable simple text quality metrics
  enable_llm: false              # Enable LLM judge assessment
  llm_provider: "openai"         # LLM provider (openai, anthropic)
  llm_model: "gpt-4o-mini"       # LLM model to use
  judge_prompt_path: "configs/judge_prompt.md"  # Path to judge prompt template

# Advanced settings for greedy decoding
advanced:
  max_concurrent_connections: 15  # Higher concurrency for greedy decoding
  connection_jitter_ms: 50        # Less jitter for faster startup
  metrics_sample_rate: 2.0        # Higher sampling rate for greedy decoding
  steady_state_window: 15         # Shorter steady state window
  steady_state_threshold: 0.05    # Higher threshold for greedy decoding
